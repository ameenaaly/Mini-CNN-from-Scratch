{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b370-10c9-4701-b7cb-82526eab8a1f",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network: Architecture, Implementation, and Results\n",
    "## Objective\n",
    "To design, implement, and evaluate a convolutional neural network (CNN) for a classification task. The report includes:\n",
    "- Description of the architecture.\n",
    "- Implementation details.\n",
    "- Results: Test accuracy and loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76376bfd-e213-477c-8713-f477974a9327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 10:07:00.260375: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist # mnist.load_data()\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60f00f-98b1-43fa-b8bf-0df1f6279fd7",
   "metadata": {},
   "source": [
    "## Loading/Preparing Dataset\n",
    "Before training the neural network, we need to load the data and prepare it:\n",
    "- Scale the input [0, 1]\n",
    "- Convert output targets into one-hot encoding.\n",
    "- Print results for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a0ce3c9-83f9-455d-a3b9-c556e6e67274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28) (60000,)\n",
      "Test data shape: (10000, 28, 28) (10000,)\n",
      "[5 0 4 1 9]\n",
      "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])]\n",
      "[7 2 1 0 4]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset and preprocess it by normalizing the pixel values.\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Check shapes\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1]\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "#initializing a list for one-hot encoded target arrayas\n",
    "y_one_hot_train = []\n",
    "y_one_hot_test = []\n",
    "\n",
    "#iterating through each target in y\n",
    "for target in y_train:\n",
    "    one_hot = np.zeros(10)  #array of 10 zeross\n",
    "    one_hot[target] = 1  \n",
    "    y_one_hot_train.append(one_hot)  # Append to the list\n",
    "\n",
    "for target in y_test:\n",
    "    one_hot = np.zeros(10)  #array of 10 zeross\n",
    "    one_hot[target] = 1  \n",
    "    y_one_hot_test.append(one_hot)  # Append to the list\n",
    "\n",
    "#print the first 5 one-hot encoded targets to verify\n",
    "print(y_train[:5])\n",
    "print(y_one_hot_train[:5])\n",
    "print(y_test[:5])\n",
    "print(y_one_hot_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8536d36-408a-4bcb-8d22-43b2901b0bd4",
   "metadata": {},
   "source": [
    "## Network Architecture and Implementation\n",
    "### Network Architecture\n",
    "The CNN consists of the following layers:\n",
    "1. **Convolutional Layer 1**:\n",
    "   - Kernel size: \\(3 * 3\\) * 2\n",
    "   - Activation: Sigmoid\n",
    "   - Output: \\(26 * 26\\) * 2\n",
    "\n",
    "2. **Average Pooling Layers**:\n",
    "   - Pool size: \\(2 * 2\\) *2\n",
    "   - Output: \\(13 * 13\\) * 2\n",
    "\n",
    "3. **1 by 1 Convolution Layer**:\n",
    "   - Flattened input size: 338\n",
    "   - Output size: Number of classes (e.g., 10 for MNIST).\n",
    "\n",
    "4. **Output Layer**:\n",
    "   - Activation: Softmax\n",
    "   - Outputs class probabilities.\n",
    "\n",
    "### Implementation\n",
    "To implement the netwrok without using extra libraries, a convolution function, a forward propagation function, an average pooling functon and a backward propagation function were written:\n",
    "1. **Convolve**: Written to relplace convolve2D function, this function recieves an input image (or feature map in other cases) and does a dot product between kernel weights and image, normalized pixel values, based on the stride and kernel sizes specified. It returns the resulting products.\n",
    "2. **Average Pooling**: Written to replace avgPool function, this function does the same function of the convolve function of iterating the input feature map, but instead of returning the dot product of the kernel and a portion of the image, it returns an average value of the region in the feature map that it parses.\n",
    "3. **Feed Forward**: This function implements the motion of an image through the CNN described above with whatever current learnable weights are in the kernels. It takes the input image, convolves it, applies a sigmoid to the convolution, then an avg pooling. It then flattens the output and puts it through 10 different 1*1 convolution kernels to get 10 different outputs. The outputs are passed to the softmax function to return probabilities which are then used to make a rrprediciton\n",
    "4. **Back Propogate**: This function is used to update the weights after every feed forward run based on cross entropy loss. It updates the weights f the 1by1 convolutions by findnig the derivative of the cross entropy loss function with respect to the softmax output to the raw logit score and finally to the 1by1 kernel weights. It further propogates backwards through the flattened output, to the pooled feature map, to the sigmoid derivative to the 3by3 kernels upsampling the data when it needs to replacing the con2dTranspose function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7972ef11-4ad6-4491-8a97-a0564ebaa1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the CNN architecture described above using only basic Python libraries such as NumPy. \n",
    "class CNN:\n",
    "    def __init__(self, learning_rate, epochs):\n",
    "        self.lr=learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability adjustment\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoidDerivative(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)  # Derivative: sigmoid(x) * (1 - sigmoid(x))\n",
    "    \n",
    "    def convolve(self, image, kernel, stride=1):\n",
    "        kernel_height, kernel_width = kernel.shape #3 by 3\n",
    "        image_height, image_width = image.shape # 28 by 28\n",
    "    \n",
    "        output_height = (image_height - kernel_height) // stride + 1 #26\n",
    "        output_width = (image_width - kernel_width) // stride + 1 #26\n",
    "    \n",
    "        output = np.zeros((output_height, output_width))\n",
    "        \n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                region = image[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width] #selects part of image\n",
    "                output[i, j] = np.sum(region * kernel) #dot product with kernel\n",
    "        return output\n",
    "    \n",
    "    def average_pooling(self, featuremap, pool_size=2, stride=2):\n",
    "        map_height, map_width = featuremap.shape\n",
    "        output_height = (map_height - pool_size) // stride + 1\n",
    "        output_width = (map_width - pool_size) // stride + 1\n",
    "    \n",
    "        output = np.zeros((output_height, output_width))\n",
    "    \n",
    "        for i in range(0, output_height):\n",
    "            for j in range(0, output_width):\n",
    "                region = featuremap[i * stride:i * stride + pool_size, j * stride:j * stride + pool_size]\n",
    "                output[i, j] = np.mean(region)\n",
    "        return output\n",
    "    \n",
    "    def flatten(self, pool1, pool2):\n",
    "        conv_output = np.stack((pool1, pool2), axis=0)\n",
    "        return conv_output.flatten()\n",
    "    \n",
    "    def feedForward(self, X): # Implement the forward and backward propagation for the network.\n",
    "        \n",
    "        # First convolutional layer feature maps after activation\n",
    "        self.conv1 = self.sigmoid(self.convolve(X, self.kernel1, stride=1)) # 26 by 26\n",
    "        self.conv2 = self.sigmoid(self.convolve(X, self.kernel2, stride=1)) #26 by 26\n",
    "    \n",
    "        # Average pooling layer\n",
    "        self.pool1 = self.average_pooling(self.conv1, 2, 2) #13 by 13\n",
    "        self.pool2 = self.average_pooling(self.conv2, 2, 2) #13 by 13\n",
    "\n",
    "        # Flatten layer\n",
    "        self.flattened_output = self.flatten(self.pool1, self.pool2)\n",
    "\n",
    "        # 1x1 convolution (fully connected layer equivalent)\n",
    "        fc_output = np.dot(self.flattened_output, self.fc_weights)  # Final concolution\n",
    "    \n",
    "        # Softmax activation\n",
    "        predictions = self.softmax(fc_output.reshape(1, -1))\n",
    "    \n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def backPropagate(self, X, y, predictions):#Use cross entropy as the error function.  \n",
    "       \n",
    "        # Compute the loss gradient (Softmax and Cross-Entropy)\n",
    "        delta_fc = predictions - y  # Shape: (1, 10)\n",
    "    \n",
    "        # Gradients for the 1x1 convolution weights\n",
    "        grad_fc_weights = np.outer(self.flattened_output, delta_fc)  # Shape: (338, 10)\n",
    "    \n",
    "        # Backpropagate to the flattened layer\n",
    "        delta_flattened = np.dot(delta_fc, self.fc_weights.T)  # Shape: (1, 338)\n",
    "    \n",
    "        # Reshape delta_flattened to match the shape of the pooled feature maps\n",
    "        delta_pool1 = delta_flattened[:, :self.pool1.size].reshape(self.pool1.shape)\n",
    "        delta_pool2 = delta_flattened[:, self.pool1.size:self.pool1.size + self.pool2.size].reshape(self.pool2.shape)\n",
    "        \n",
    "        # Upsample gradients from the pooling layer to match the convolutional layer\n",
    "        def upsample(gradient, original_shape, pool_size, stride):\n",
    "            upsampled = np.zeros(original_shape)\n",
    "            for i in range(gradient.shape[0]):\n",
    "                for j in range(gradient.shape[1]):\n",
    "                    upsampled[i * stride:i * stride + pool_size, j * stride:j * stride + pool_size] = gradient[i, j] / (pool_size * pool_size)\n",
    "            return upsampled\n",
    "            \n",
    "        grad_conv1 = upsample(delta_pool1, self.conv1.shape, pool_size=2, stride=2) * self.sigmoidDerivative(self.conv1)\n",
    "        grad_conv2 = upsample(delta_pool2, self.conv2.shape, pool_size=2, stride=2) * self.sigmoidDerivative(self.conv2)\n",
    "    \n",
    "        # Gradients for the convolutional kernels\n",
    "        grad_kernel1 = np.zeros_like(self.kernel1)\n",
    "        grad_kernel2 = np.zeros_like(self.kernel2)\n",
    "    \n",
    "        for i in range(grad_kernel1.shape[0]):\n",
    "            for j in range(grad_kernel1.shape[1]):\n",
    "                grad_kernel1[i, j] = np.sum(\n",
    "                    X[i:i + grad_conv1.shape[0], j:j + grad_conv1.shape[1]] * grad_conv1\n",
    "                )\n",
    "                grad_kernel2[i, j] = np.sum(\n",
    "                    X[i:i + grad_conv2.shape[0], j:j + grad_conv2.shape[1]] * grad_conv2\n",
    "                )\n",
    "    \n",
    "        # Update weights\n",
    "        self.fc_weights -= self.lr * grad_fc_weights\n",
    "        self.kernel1 -= self.lr * grad_kernel1\n",
    "        self.kernel2 -= self.lr * grad_kernel2\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train): # Train the network using the training dataset and evaluate its performance using the test dataset.\n",
    "        # Initialize random weights for convolutional kernels (first layer kernels and 1by1 convolutions in end)\n",
    "        self.kernel1 = np.random.randn(3, 3) * 0.1  #3 by 3\n",
    "        self.kernel2 = np.random.randn(3, 3) * 0.1  #3 by 3 \n",
    "        self.fc_weights = np.random.randn(338, 10) * 0.1  # Random weights for final convolution 10 output channels\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for X, y in zip(X_train, y_train):\n",
    "                predictions = self.feedForward(X)\n",
    "                self.backPropagate(X, y, predictions)\n",
    "         \n",
    "           \n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        correct = 0\n",
    "        total = X_test.shape[0]\n",
    "        total_loss = 0.0\n",
    "    \n",
    "        for i in range(total):\n",
    "            # Forward pass for a single sample\n",
    "            y_pred = self.feedForward(X_test[i])  # Shape: (1, num_classes)\n",
    "            y_pred_class = np.argmax(y_pred)  # Predicted class\n",
    "            y_true_class = np.argmax(y_test[i])  # True class\n",
    "            \n",
    "            # Check if the prediction is correct\n",
    "            if y_pred_class == y_true_class:\n",
    "                correct += 1\n",
    "            \n",
    "            # Compute cross-entropy loss for this sample\n",
    "            loss = -np.sum(y_test[i] * np.log(y_pred + 1e-8))  # Add small value to avoid log(0)\n",
    "            total_loss += loss\n",
    "    \n",
    "        # Calculate accuracy\n",
    "        accuracy = correct / total\n",
    "        # Calculate average loss\n",
    "        avg_loss = total_loss / total\n",
    "    \n",
    "        print(f\"Accuracy: {accuracy:.2f}, Average Loss: {avg_loss:.4f}\")\n",
    "        return accuracy, avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10da6b-fb39-4a22-bda0-16e1f7d3e72f",
   "metadata": {},
   "source": [
    "## Training and Test Results:\n",
    "The network was trained using stochastic gradient descent. A learning rate of 0.001 was used and a total of 10 epochs. The network was evaluated on the test dataset using accuracy and average cross-entropy loss as metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c98eec06-990b-4b8b-9a9c-fb5e5ecbf886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89, Average Loss: 0.3695\n",
      "Accuracy: 0.89, Average Loss: 0.3695\n"
     ]
    }
   ],
   "source": [
    "#Accuracy and loss achieved by the network on the test dataset\n",
    "myCNN = CNN(0.001, 10)\n",
    "myCNN.train(X_train, y_one_hot_train)\n",
    "accuracy, avg_loss = myCNN.evaluate(X_test, y_one_hot_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}, Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52e1ef-aacd-41cd-95cb-6f239625b8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
